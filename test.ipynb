{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17c0ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "model_id=\"google/gemma-3-1b-it\"\n",
    "# model_id=\"Qwen/Qwen3Guard-Gen-4B\"\n",
    "device=\"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, dtype=\"auto\", device_map=device)\n",
    "\n",
    "# Try restarting your kernel if having issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1911ccc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'What is your name?\\n\\n(Pause for response)\\n\\nOkay, thank you. I’m Alex.\\n\\nHow are you doing today?\\n\\n(Pause for response)\\n\\nI’m doing well, thanks for asking'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_pipeline = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "generation_pipeline(\"What is your name?\", max_new_tokens=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5598f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2,   3689,    563, 236743, 236770,    900, 236743, 236770, 236881],\n",
      "        [     0,      0,      2,    818,   5279,    529,    506,   2590,    563],\n",
      "        [     0,      0,      0,      0,      0,      2, 148673,    793,   1044]])\n"
     ]
    }
   ],
   "source": [
    "input_prompt = [\n",
    "    \"What is 1 + 1?\",\n",
    "    \"The capital of the US is\",\n",
    "    \"gibberish\"\n",
    "]\n",
    "\n",
    "tokenized = tokenizer(input_prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "print(tokenized[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3950cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful AI assitant that helps engineers and real estate developers\n",
      "\n",
      "Where does the sun rise?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "A developer is<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_template = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful AI assitant that helps engineers and real estate developers\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Where does the sun rise?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"A developer is\"\n",
    "    }\n",
    "]\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenized = tokenizer.apply_chat_template(\n",
    "    prompt_template,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(tokenized)\n",
    "# Note, google/gemma-3-1b-it does not use system roles. The system prompt is injected directly into the first user message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d324b130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2,    105,   2364,    107,   3048,    659,    496,  11045,  12498,\n",
      "           1188,  42563,    600,   7351,  22072,    532,   1759,   9350,  18270,\n",
      "            108,  10936,   1677,    506,   3768,   8570, 236881,    106,    107,\n",
      "            105,   4368,    107, 215784, 140871]])\n"
     ]
    }
   ],
   "source": [
    "prompt_template = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful AI assitant that helps engineers and real estate developers\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Calculate the design occupant load for a 500 sq ft Library Reading Room in a new university building in Manhattan, per NYC 2022 Building Code.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Aye Aye\"\n",
    "    }\n",
    "]\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenized = tokenizer.apply_chat_template(\n",
    "    prompt_template,\n",
    "    add_generation_prompt=False,\n",
    "    continue_final_message=True,\n",
    "    tokenize=True,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(tokenized)\n",
    "# Note, google/gemma-3-1b-it does not use system roles. The system prompt is injected directly into the first user message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e8c5baa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful AI assitant that helps engineers and real estate developers\n",
      "\n",
      "Where does the sun rise?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Aye Aye, Captain! That's a fantastic question, and one that's been pondered by sailors and\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(tokenized.to(device), max_new_tokens=20)\n",
    "decoded = tokenizer.batch_decode(out)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8894c6a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tokenizer.batch_decode(\u001b[43mtokenized\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[31mTypeError\u001b[39m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "tokenizer.batch_decode(tokenized[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48603c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\borow\\AppData\\Local\\Temp\\ipykernel_31904\\2594409403.py:1: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\python_variable_indexing.cpp:351.)\n",
      "  tokenized[\"attention_mask\"]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtokenized\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mattention_mask\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "tokenized[\"attention_mask\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
