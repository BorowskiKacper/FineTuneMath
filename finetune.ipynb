{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb2d1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab setup\n",
    "!pip install transformers datasets peft accelerate bitsandbytes scikit-learn -q\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "\n",
    "# Authenticate with Hugging Face to use Gemma\n",
    "from huggingface_hub import login\n",
    "login()  # You'll need a HF token with access to Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be9b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ds = load_dataset(\"HuggingFaceH4/MATH-500\", split=\"test\")\n",
    "df = ds.to_pandas()\n",
    "\n",
    "# Split into train and test\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "df_train.head()\n",
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f19031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id=\"google/gemma-3-1b-it\"\n",
    "device=\"cuda\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token   # required for Gemma\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    dtype=torch.float32 if device == \"cpu\" else torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "print(f\"dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e9e6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a helpful math assistant that solves problems step by step.\"\n",
    "}\n",
    "\n",
    "USER_MESSAGES = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": row.problem  \n",
    "    }\n",
    "    for _, row in df_train.iloc[:5].iterrows()\n",
    "]\n",
    "\n",
    "POST_MESSAGE = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Solution: \"\n",
    "    }\n",
    "\n",
    "\n",
    "PROMPTS = [\n",
    "    [SYSTEM_PROMPT, USER_MSG, POST_MESSAGE]\n",
    "    for USER_MSG in USER_MESSAGES\n",
    "]\n",
    "\n",
    "print(json.dumps(PROMPTS[0], indent=4))\n",
    "\n",
    "tokenized = tokenizer.apply_chat_template(\n",
    "    PROMPTS,\n",
    "    continue_final_message=True,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "out = model.generate(tokenized, max_new_tokens=14)\n",
    "\n",
    "decoded = tokenizer.batch_decode(out)\n",
    "print(decoded)\n",
    "\n",
    "labels = [(d.split(\"\\nSolution:\")[-1]).strip() for d in decoded]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113fb675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_sample(conversation, target_response, max_length=512):\n",
    "    \"\"\"\n",
    "    Generate a single training sample with proper label masking.\n",
    "    \n",
    "    For causal LM fine-tuning, we want:\n",
    "    - input_ids: the full sequence (prompt + response)\n",
    "    - labels: same as input_ids, but with -100 for tokens we don't want to compute loss on (the prompt)\n",
    "    \n",
    "    Args:\n",
    "        conversation: list of message dicts with role/content (the prompt)\n",
    "        target_response: the expected model output\n",
    "        max_length: maximum sequence length\n",
    "    \n",
    "    Returns:\n",
    "        dict with input_ids, attention_mask, labels\n",
    "    \"\"\"\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        conversation,\n",
    "        continue_final_message=True,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    full_text = prompt_text + target_response + tokenizer.eos_token\n",
    "    \n",
    "    full_encoding = tokenizer(\n",
    "        full_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        add_special_tokens=False  # chat_template already added special tokens\n",
    "    )\n",
    "    \n",
    "    # Tokenize just the prompt to find where labels should start\n",
    "    prompt_encoding = tokenizer(\n",
    "        prompt_text,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    prompt_length = prompt_encoding[\"input_ids\"].shape[1]\n",
    "    \n",
    "    # Create labels: -100 for prompt tokens (ignore in loss), actual tokens for response\n",
    "    labels = full_encoding[\"input_ids\"].clone()\n",
    "    labels[0, :prompt_length] = -100  # Mask the prompt\n",
    "    labels[labels == tokenizer.pad_token_id] = -100  # Also mask padding\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": full_encoding[\"input_ids\"],\n",
    "        \"attention_mask\": full_encoding[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a693d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful math assistant that solves problems step by step.\"},\n",
    "    {\"role\": \"user\", \"content\": df_train.iloc[0].problem},  \n",
    "    {\"role\": \"assistant\", \"content\": \"Solution: \"}\n",
    "]\n",
    "test_response = df_train.iloc[0].solution\n",
    "\n",
    "sample = generate_training_sample(test_conversation, test_response)\n",
    "print(f\"Input IDs shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Labels shape: {sample['labels'].shape}\")\n",
    "print(f\"Non-masked label tokens: {(sample['labels'] != -100).sum().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d25392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful math assistant that solves problems step by step.\"},\n",
    "            {\"role\": \"user\", \"content\": row.problem},\n",
    "            {\"role\": \"assistant\", \"content\": \"Solution: \"}\n",
    "        ]\n",
    "        \n",
    "        target = row.solution\n",
    "        \n",
    "        sample = generate_training_sample(conversation, target, self.max_length)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": sample[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": sample[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": sample[\"labels\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "# Create dataset\n",
    "dataset = MathDataset(df_train, tokenizer, max_length=512)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Test one sample\n",
    "test_item = dataset[0]\n",
    "print(f\"Sample input_ids shape: {test_item['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f07233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Configure LoRA for Gemma\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,  # Rank of the update matrices (lower = fewer params, higher = more capacity)\n",
    "    lora_alpha=16,  # Scaling factor\n",
    "    lora_dropout=0.1,  # Dropout for LoRA layers\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Attention projection layers\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b7c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCHS = 3\n",
    "GRADIENT_ACCUMULATION_STEPS = 1  # Effective batch size = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "optimizer = AdamW(\n",
    "    filter(lambda p: p.requires_grad, peft_model.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(dataset)}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Steps per epoch: {len(train_loader)}\")\n",
    "print(f\"Total training steps: {len(train_loader) * NUM_EPOCHS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5267f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "peft_model.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # Move batch to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = peft_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss / GRADIENT_ACCUMULATION_STEPS\n",
    "        total_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights every GRADIENT_ACCUMULATION_STEPS\n",
    "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item() * GRADIENT_ACCUMULATION_STEPS:.4f}\"})\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1} - Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa44367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and save there\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/gemma-math-lora\"\n",
    "peft_model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Model saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a8596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference with the fine-tuned model\n",
    "peft_model.eval()\n",
    "\n",
    "def generate_response(problem, max_new_tokens=256):\n",
    "    \"\"\"Generate a response for a given math problem.\"\"\"\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful math assistant that solves problems step by step.\"},\n",
    "        {\"role\": \"user\", \"content\": problem},\n",
    "        {\"role\": \"assistant\", \"content\": \"Solution: \"}\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        conversation,\n",
    "        continue_final_message=True,\n",
    "        tokenize=False\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = peft_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # Deterministic for testing\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract just the assistant's response\n",
    "    return response.split(\"Solution: \")[-1]\n",
    "\n",
    "# Test with a sample from the MATH-500 dataset\n",
    "test_problem = df.iloc[0].problem\n",
    "print(f\"Problem: {test_problem}\\n\")\n",
    "print(f\"Expected answer: {df.iloc[0].answer}\\n\")\n",
    "print(f\"Model response:\\n{generate_response(test_problem)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
