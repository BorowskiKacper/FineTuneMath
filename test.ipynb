{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c0ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "model_id=\"google/gemma-3-1b-it\"\n",
    "# model_id=\"Qwen/Qwen3Guard-Gen-4B\"\n",
    "device=\"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, dtype=\"auto\", device_map=device)\n",
    "\n",
    "# Try restarting your kernel if having issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1911ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_pipeline = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "generation_pipeline(\"What is your name?\", max_new_tokens=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5598f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = [\n",
    "    \"What is 1 + 1?\",\n",
    "    \"The capital of the US is\",\n",
    "    \"gibberish\"\n",
    "]\n",
    "\n",
    "tokenized = tokenizer(input_prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "print(tokenized[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3950cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful AI assitant that helps engineers and real estate developers\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Where does the sun rise?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"A developer is\"\n",
    "    }\n",
    "]\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenized = tokenizer.apply_chat_template(\n",
    "    prompt_template,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(tokenized)\n",
    "# Note, google/gemma-3-1b-it does not use system roles. The system prompt is injected directly into the first user message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d324b130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2,    105,   2364,    107,   3048,    659,    496,  11045,  12498,\n",
      "           1188,  42563,    600,   7351,  22072,    532,   1759,   9350,  18270,\n",
      "            108,  50702,    506,   1702, 109616,   3711,    573,    496, 236743,\n",
      "         236810, 236771, 236771,  18288,  12110,  13826,  22033,  18120,    528,\n",
      "            496,    861,  10467,   3788,    528,  35549, 236764,    810,  46585,\n",
      "         236743, 236778, 236771, 236778, 236778,  16087,   8739, 236761,    106,\n",
      "            107,    105,   4368,    107,  47875,    522, 236787]])\n"
     ]
    }
   ],
   "source": [
    "prompt_template = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful AI assitant that helps engineers and real estate developers\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Calculate the design occupant load for a 500 sq ft Library Reading Room in a new university building in Manhattan, per NYC 2022 Building Code.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Reasoning: \"\n",
    "    }\n",
    "]\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenized = tokenizer.apply_chat_template(\n",
    "    prompt_template,\n",
    "    add_generation_prompt=False,\n",
    "    continue_final_message=True,\n",
    "    tokenize=True,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(tokenized)\n",
    "# Note, google/gemma-3-1b-it does not use system roles. The system prompt is injected directly into the first user message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c5baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(tokenized.to(device), max_new_tokens=80)\n",
    "decoded = tokenizer.batch_decode(out)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b448781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello how are\"\n",
    "input_ids = tokenizer([text], return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "out = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630b4713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "probability_dist = nn.Softmax()(out.logits[0, -1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1555bf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_dist[611]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43624b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(probability_dist)\n",
    "probability_dist.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f2339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(223130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce777fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"â–you\")\n",
    "# tokenizer.vocab[\"you\"] \n",
    "\n",
    "# probability_dist.shape\n",
    "# torch.max(probability_dist)\n",
    "\n",
    "# for key, val in probability_dist.items():\n",
    "#     if val > 0.1:\n",
    "#         print(key)\n",
    "#         print(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5f844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE_DIMENSION = -1\n",
    "BATCH_INDEX = 0\n",
    "\n",
    "out.logits.argmax(axis=VOCABULARY_SIZE_DIMENSION)[BATCH_INDEX, -1]\n",
    "# out.logits.argmax(axis=-1)[0, -1]\n",
    "# out.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5568914",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenized[:, :-1]\n",
    "target_ids = tokenized[:, 1:]\n",
    "\n",
    "print(input_ids)\n",
    "print(target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2488464",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = prompt_template[-1][\"content\"]\n",
    "print(answer)\n",
    "labels_tokenized = tokenizer([\" \" + answer], add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"]\n",
    "print(labels_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "077abe9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "              1,      1, 121817, 236787, 236743,      1]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_tokenized = tokenizer([\" \" + answer + tokenizer.eos_token],\n",
    "                             add_special_tokens=False, return_tensors=\"pt\", padding=\"max_length\", max_length=target_ids.shape[1])[\"input_ids\"]\n",
    "labels_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "18e14fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100, 121817, 236787, 236743,      1]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_tokenized_fixed = torch.where(labels_tokenized == tokenizer.pad_token_id, -100, labels_tokenized)\n",
    "labels_tokenized_fixed[:,-1] = tokenizer.eos_token_id\n",
    "labels_tokenized_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65cb9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = \"\"\"Reasoning: Per NYC Building Code Table 1004.5, the occupant load factor for library reading rooms is 1 person per 50 net sq ft. \n",
    "Calculation: 500 / 50 = 10.0. \n",
    "\n",
    "Answer: 10 occupants\"\"\"\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe816e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_output_pair(prompt, target_responses):\n",
    "    chat_templates = tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        continue_final_message=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "    print(chat_templates)\n",
    "    # full_response_text = [\n",
    "    #     (chat_template + \" \" + target_response + tokenizer.eos)\n",
    "    # ]\n",
    "\n",
    "\n",
    "generate_input_output_pair(prompt_template, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
