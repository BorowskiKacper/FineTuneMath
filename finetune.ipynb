{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3be9b8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>solution</th>\n",
       "      <th>answer</th>\n",
       "      <th>subject</th>\n",
       "      <th>level</th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Find the unique $\\textbf{odd}$ integer $t$ suc...</td>\n",
       "      <td>We could find the answer by trial and error --...</td>\n",
       "      <td>17</td>\n",
       "      <td>Number Theory</td>\n",
       "      <td>4</td>\n",
       "      <td>test/number_theory/1065.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>Convert $\\frac{57}{160}$ to a terminating deci...</td>\n",
       "      <td>A terminating decimal can be written in the fo...</td>\n",
       "      <td>.35625</td>\n",
       "      <td>Number Theory</td>\n",
       "      <td>2</td>\n",
       "      <td>test/number_theory/410.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Let $a$ be a positive real number such that al...</td>\n",
       "      <td>Note that $x = -1$ is always a root of $x^3 + ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Intermediate Algebra</td>\n",
       "      <td>3</td>\n",
       "      <td>test/intermediate_algebra/1000.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>Let $f(x) = x - 3$ and $q(x) = bx +1$.  If $f(...</td>\n",
       "      <td>We have $q(1) = b\\cdot 1 + 1 = b+1$, so $f(q(1...</td>\n",
       "      <td>-1</td>\n",
       "      <td>Algebra</td>\n",
       "      <td>3</td>\n",
       "      <td>test/algebra/1936.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>Let $x,$ $y,$ and $z$ be positive real numbers...</td>\n",
       "      <td>We can write $(x + y)(y + z)$ as $xz + y(x + y...</td>\n",
       "      <td>2</td>\n",
       "      <td>Intermediate Algebra</td>\n",
       "      <td>4</td>\n",
       "      <td>test/intermediate_algebra/190.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               problem  \\\n",
       "249  Find the unique $\\textbf{odd}$ integer $t$ suc...   \n",
       "433  Convert $\\frac{57}{160}$ to a terminating deci...   \n",
       "19   Let $a$ be a positive real number such that al...   \n",
       "322  Let $f(x) = x - 3$ and $q(x) = bx +1$.  If $f(...   \n",
       "332  Let $x,$ $y,$ and $z$ be positive real numbers...   \n",
       "\n",
       "                                              solution  answer  \\\n",
       "249  We could find the answer by trial and error --...      17   \n",
       "433  A terminating decimal can be written in the fo...  .35625   \n",
       "19   Note that $x = -1$ is always a root of $x^3 + ...       3   \n",
       "322  We have $q(1) = b\\cdot 1 + 1 = b+1$, so $f(q(1...      -1   \n",
       "332  We can write $(x + y)(y + z)$ as $xz + y(x + y...       2   \n",
       "\n",
       "                  subject  level                            unique_id  \n",
       "249         Number Theory      4         test/number_theory/1065.json  \n",
       "433         Number Theory      2          test/number_theory/410.json  \n",
       "19   Intermediate Algebra      3  test/intermediate_algebra/1000.json  \n",
       "322               Algebra      3               test/algebra/1936.json  \n",
       "332  Intermediate Algebra      4   test/intermediate_algebra/190.json  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ds = load_dataset(\"HuggingFaceH4/MATH-500\", split=\"test\")\n",
    "df = ds.to_pandas()\n",
    "\n",
    "# Split into train and test\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "df_train.head()\n",
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f19031a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id=\"google/gemma-3-1b-it\"\n",
    "device=\"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token   # required for Gemma\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    dtype=torch.float32 if device == \"cpu\" else torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "print(f\"dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6e9e6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are a helpful math assistant that solves problems step by step.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"Find the unique $\\\\textbf{odd}$ integer $t$ such that $0<t<23$ and $t+2$ is the inverse of $t$ modulo $23$.\"\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Solution: \"\n",
      "    }\n",
      "]\n",
      "['<bos><start_of_turn>user\\nYou are a helpful math assistant that solves problems step by step.\\n\\nFind the unique $\\\\textbf{odd}$ integer $t$ such that $0<t<23$ and $t+2$ is the inverse of $t$ modulo $23$.<end_of_turn>\\n<start_of_turn>model\\nSolution:<eos><eos><eos><eos><eos><eos><eos><eos>methode\\nLet $t$ be an odd integer such that $', '<bos><start_of_turn>user\\nYou are a helpful math assistant that solves problems step by step.\\n\\nConvert $\\\\frac{57}{160}$ to a terminating decimal.<end_of_turn>\\n<start_of_turn>model\\nSolution:<eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>', '<bos><start_of_turn>user\\nYou are a helpful math assistant that solves problems step by step.\\n\\nLet $a$ be a positive real number such that all the roots of\\n\\\\[x^3 + ax^2 + ax + 1 = 0\\\\]are real.  Find the smallest possible value of $a.$<end_of_turn>\\n<start_of_turn>model\\nSolution:<eos><eos>\\nLet the given cubic equation be $P(x) = x', '<bos><start_of_turn>user\\nYou are a helpful math assistant that solves problems step by step.\\n\\nLet $f(x) = x - 3$ and $q(x) = bx +1$.  If $f(q(1)) = -3$, what is $b$?<end_of_turn>\\n<start_of_turn>model\\nSolution:<eos><eos><eos><eos><eos><eos><eos>alternatively, we are given that $f(x) = x', '<bos><start_of_turn>user\\nYou are a helpful math assistant that solves problems step by step.\\n\\nLet $x,$ $y,$ and $z$ be positive real numbers such that $xyz(x + y + z) = 1.$  Find the minimum value of\\n\\\\[(x + y)(y + z).\\\\]<end_of_turn>\\n<start_of_turn>model\\nSolution:\\nLet $x, y, z$ be positive real numbers such']\n",
      "['<eos><eos><eos><eos><eos><eos><eos><eos>methode\\nLet $t$ be an odd integer such that $', '<eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><eos><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>', '<eos><eos>\\nLet the given cubic equation be $P(x) = x', '<eos><eos><eos><eos><eos><eos><eos>alternatively, we are given that $f(x) = x', 'Let $x, y, z$ be positive real numbers such']\n"
     ]
    }
   ],
   "source": [
    "# SYSTEM_PROMPT = {\n",
    "#     \"role\": \"system\",\n",
    "#     \"content\": \"You are a helpful math assistant that solves problems step by step.\"\n",
    "# \n",
    "SYSTEM_PROMPT = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a helpful math assistant that solves problems step by step.\"\n",
    "}\n",
    "\n",
    "USER_MESSAGES = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": row.problem  \n",
    "    }\n",
    "    for _, row in df_train.iloc[:5].iterrows()\n",
    "]\n",
    "\n",
    "POST_MESSAGE = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Solution: \"\n",
    "    }\n",
    "\n",
    "\n",
    "PROMPTS = [\n",
    "    [SYSTEM_PROMPT, USER_MSG, POST_MESSAGE]\n",
    "    for USER_MSG in USER_MESSAGES\n",
    "]\n",
    "\n",
    "print(json.dumps(PROMPTS[0], indent=4))\n",
    "\n",
    "tokenized = tokenizer.apply_chat_template(\n",
    "    PROMPTS,\n",
    "    continue_final_message=True,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "out = model.generate(tokenized, max_new_tokens=14)\n",
    "\n",
    "decoded = tokenizer.batch_decode(out)\n",
    "print(decoded)\n",
    "\n",
    "labels = [(d.split(\"\\nSolution:\")[-1]).strip() for d in decoded]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "113fb675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_sample(conversation, target_response, max_length=512):\n",
    "    \"\"\"\n",
    "    Generate a single training sample with proper label masking.\n",
    "    \n",
    "    For causal LM fine-tuning, we want:\n",
    "    - input_ids: the full sequence (prompt + response)\n",
    "    - labels: same as input_ids, but with -100 for tokens we don't want to compute loss on (the prompt)\n",
    "    \n",
    "    Args:\n",
    "        conversation: list of message dicts with role/content (the prompt)\n",
    "        target_response: the expected model output\n",
    "        max_length: maximum sequence length\n",
    "    \n",
    "    Returns:\n",
    "        dict with input_ids, attention_mask, labels\n",
    "    \"\"\"\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        conversation,\n",
    "        continue_final_message=True,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    full_text = prompt_text + target_response + tokenizer.eos_token\n",
    "    \n",
    "    full_encoding = tokenizer(\n",
    "        full_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        add_special_tokens=False  # chat_template already added special tokens\n",
    "    )\n",
    "    \n",
    "    # Tokenize just the prompt to find where labels should start\n",
    "    prompt_encoding = tokenizer(\n",
    "        prompt_text,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    prompt_length = prompt_encoding[\"input_ids\"].shape[1]\n",
    "    \n",
    "    # Create labels: -100 for prompt tokens (ignore in loss), actual tokens for response\n",
    "    labels = full_encoding[\"input_ids\"].clone()\n",
    "    labels[0, :prompt_length] = -100  # Mask the prompt\n",
    "    labels[labels == tokenizer.pad_token_id] = -100  # Also mask padding\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": full_encoding[\"input_ids\"],\n",
    "        \"attention_mask\": full_encoding[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a693d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([1, 512])\n",
      "Labels shape: torch.Size([1, 512])\n",
      "Non-masked label tokens: 238\n"
     ]
    }
   ],
   "source": [
    "test_conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful math assistant that solves problems step by step.\"},\n",
    "    {\"role\": \"user\", \"content\": df_train.iloc[0].problem},  \n",
    "    {\"role\": \"assistant\", \"content\": \"Solution: \"}\n",
    "]\n",
    "test_response = df_train.iloc[0].solution\n",
    "\n",
    "sample = generate_training_sample(test_conversation, test_response)\n",
    "print(f\"Input IDs shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Labels shape: {sample['labels'].shape}\")\n",
    "print(f\"Non-masked label tokens: {(sample['labels'] != -100).sum().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74d25392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 400\n",
      "Sample input_ids shape: torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MathDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful math assistant that solves problems step by step.\"},\n",
    "            {\"role\": \"user\", \"content\": row.problem},\n",
    "            {\"role\": \"assistant\", \"content\": \"Solution: \"}\n",
    "        ]\n",
    "        \n",
    "        target = row.solution\n",
    "        \n",
    "        sample = generate_training_sample(conversation, target, self.max_length)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": sample[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": sample[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": sample[\"labels\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "# Create dataset\n",
    "dataset = MathDataset(df_train, tokenizer, max_length=512)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Test one sample\n",
    "test_item = dataset[0]\n",
    "print(f\"Sample input_ids shape: {test_item['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82f07233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,490,944 || all params: 1,001,376,896 || trainable%: 0.1489\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Configure LoRA for Gemma\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,  # Rank of the update matrices (lower = fewer params, higher = more capacity)\n",
    "    lora_alpha=16,  # Scaling factor\n",
    "    lora_dropout=0.1,  # Dropout for LoRA layers\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Attention projection layers\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e9b7c749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 400\n",
      "Batch size: 2\n",
      "Steps per epoch: 200\n",
      "Total training steps: 600\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCHS = 3\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Effective batch size = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "optimizer = AdamW(\n",
    "    filter(lambda p: p.requires_grad, peft_model.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(dataset)}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Steps per epoch: {len(train_loader)}\")\n",
    "print(f\"Total training steps: {len(train_loader) * NUM_EPOCHS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5267f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "peft_model.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # Move batch to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = peft_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss / GRADIENT_ACCUMULATION_STEPS\n",
    "        total_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights every GRADIENT_ACCUMULATION_STEPS\n",
    "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item() * GRADIENT_ACCUMULATION_STEPS:.4f}\"})\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1} - Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa44367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned LoRA adapter\n",
    "OUTPUT_DIR = \"./gemma-math-lora\"\n",
    "peft_model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Model saved to {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a8596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference with the fine-tuned model\n",
    "peft_model.eval()\n",
    "\n",
    "def generate_response(problem, max_new_tokens=256):\n",
    "    \"\"\"Generate a response for a given math problem.\"\"\"\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful math assistant that solves problems step by step.\"},\n",
    "        {\"role\": \"user\", \"content\": problem},\n",
    "        {\"role\": \"assistant\", \"content\": \"Solution: \"}\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        conversation,\n",
    "        continue_final_message=True,\n",
    "        tokenize=False\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = peft_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # Deterministic for testing\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract just the assistant's response\n",
    "    return response.split(\"Solution: \")[-1]\n",
    "\n",
    "# Test with a sample from the MATH-500 dataset\n",
    "test_problem = df.iloc[0].problem\n",
    "print(f\"Problem: {test_problem}\\n\")\n",
    "print(f\"Expected answer: {df.iloc[0].answer}\\n\")\n",
    "print(f\"Model response:\\n{generate_response(test_problem)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
